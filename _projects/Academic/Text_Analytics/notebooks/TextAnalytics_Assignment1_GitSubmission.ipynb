{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Analytics\n",
    "\n",
    "\n",
    "# Assignment 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Submitted by:*  \n",
    "\n",
    "**Zena Drakou | Marissa Hausman | Hitesh Prabhu | Chase Slocum | Yawen Ye - MSBA 2017 **\n",
    "\n",
    "**Sept 6, 2016**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initial Setup:\n",
    "\n",
    "1. Importing libraries\n",
    "2. Reading dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas import Series, DataFrame\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "import random\n",
    "import string\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "job = pd.read_csv(\"files\\Train_rev1.csv\")\n",
    "salary = job[\"SalaryNormalized\"]\n",
    "\n",
    "quantile_75 = salary.quantile(q = 0.75)\n",
    "job[\"salary_category\"] = \"\"\n",
    "job.loc[job[\"SalaryNormalized\"] >= quantile_75, \"salary_category\"] = \"High Salary\"\n",
    "job.loc[job[\"SalaryNormalized\"] < quantile_75, \"salary_category\"] = \"Low Salary\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating sample size to run the notebook on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "subset_size = 10000\n",
    "feature_set = subset_size/2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part A "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A1 - What are the top 5 parts of speech in this corpus of job descriptions? How frequently do they appear?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('NN', 626738),\n",
       " ('JJ', 274481),\n",
       " ('IN', 258077),\n",
       " ('DT', 193658),\n",
       " ('NNS', 191164)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating 2 lists 'des' and 'cat' for the description and the category respectively\n",
    "descriptions = []\n",
    "categories = []\n",
    "\n",
    "# Function to encode the text to 'utf-8' before tokenization\n",
    "def tolist_utf8(h):\n",
    "    descriptions.append(h.decode('utf-8').lower())\n",
    "\n",
    "# put the values from salary_category column into list\n",
    "def tolist(h):\n",
    "    categories.append(h)   \n",
    "\n",
    "dess = job[\"FullDescription\"]\n",
    "dess.map(tolist_utf8)\n",
    "\n",
    "cate = job[\"salary_category\"]\n",
    "cate.map(tolist)\n",
    "\n",
    "descriptions_subset = []\n",
    "\n",
    "# picking a subset of the job-descriptions dataset\n",
    "for i in range(subset_size):\n",
    "    descriptions_subset.append(nltk.word_tokenize(descriptions[i]))\n",
    "\n",
    "# Creating one list with all the words \n",
    "jobdes_bow = []\n",
    "for i in range(subset_size):\n",
    "    for j in range(len(descriptions_subset[i])):\n",
    "        jobdes_bow.append(descriptions_subset[i][j])\n",
    "\n",
    "jobdes_pos_tagged = nltk.pos_tag(jobdes_bow)\n",
    "jobdes_pos_tagged_freq = nltk.FreqDist(tag_single for (word, tag_single) in jobdes_pos_tagged)\n",
    "jobdes_pos_tagged_freq.most_common(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_The top 5 parts of speech were found to be Nouns (singular), Adjectives, Preposition/conjunctions, Determiners and Common Nouns (Plural)_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A2 - Zipf's law: Does this corpus support Zipfâ€™s law? Plot the most common 100 words in the corpus against the theoretical prediction of the law. For this question, do not remove stopwords. Also do not perform stemming or lemmatization.\n",
    "\n",
    "#### Code reference from this link: \n",
    "https://www.garysieling.com/blog/exploring-zipfs-law-with-python-nltk-scipy-and-matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.          0.91329394]\n",
      " [ 0.91329394  1.        ]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x854e0438>]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX4AAAD8CAYAAABw1c+bAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\niFqixVM5agAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x806634a8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "top100_wordcount_list = []\n",
    "jobdes_word_freq = nltk.FreqDist(word for (word, tag_single) in jobdes_pos_tagged)\n",
    "\n",
    "# putting the most common 100 words with their counts into a list\n",
    "for word, frequency in jobdes_word_freq.most_common(100):\n",
    "    top100_wordcount_list.append((word, frequency))\n",
    "\n",
    "import scipy.stats as ss\n",
    " \n",
    "amb = [(w, c, len(wordnet.synsets(w))) for (w, c) in top100_wordcount_list if len(wordnet.synsets(w)) > 0]\n",
    " \n",
    "amb_p_rank = ss.rankdata([p for (w, c, p) in amb])\n",
    "amb_c_rank = ss.rankdata([c for (w, c, p) in amb])\n",
    " \n",
    "amb_ranked = zip(amb, amb_p_rank, amb_c_rank)\n",
    " \n",
    "print np.corrcoef(amb_c_rank, [math.log(c) for (w, c, p) in amb])\n",
    "\n",
    "import matplotlib\n",
    "rev = [100-r+1 for r in amb_c_rank]\n",
    "\n",
    "plt.plot([math.log(c) for c in rev], [math.log(c) for (w, c, p) in amb], 'ro')\n",
    "\n",
    "# plt.plot([c for (w,c,p) in amb], [p for (w,c,p) in amb], 'bs')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_This corpus follows Zipf's Law_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A3: If we remove stopwords and lemmatize the corpus, what are the 10 most common words? What is their frequency?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "experience: 17298\n",
      "work: 12311\n",
      "care: 11975\n",
      "role: 11796\n",
      "client: 11600\n",
      "team: 11055\n",
      "working: 10014\n",
      "service: 9463\n",
      "within: 9245\n",
      "manager: 9103\n"
     ]
    }
   ],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "# remove it if you need punctuation\n",
    "stop_words.update(['.', ',', '\"', \"'\", '?', '!', ':', ';', '(', ')', '[', ']', '{', '}','****']) \n",
    "\n",
    "jobdes_cleaned = []\n",
    "jobdes_lemmatized = []\n",
    "jobdes_nostopwords = []\n",
    "\n",
    "# pick a small subset of the dataset\n",
    "for i in range(subset_size):\n",
    "    word_tokenize = nltk.word_tokenize(descriptions[i])\n",
    "    lemma_word = map(lambda x: lemmatizer.lemmatize(x), word_tokenize)\n",
    "    jobdes_lemmatized.append(lemma_word)\n",
    "    no_stop = filter(lambda x: x not in stop_words, word_tokenize)\n",
    "    jobdes_nostopwords.append(no_stop)\n",
    "    cleaned_text = filter(lambda x: x not in stop_words, lemma_word)\n",
    "    jobdes_cleaned.append(cleaned_text)\n",
    "\n",
    "jobdes_bow_cleaned = []\n",
    "jobdes_bow_lemmatized = []\n",
    "jobdes_bow_nostopwords = []\n",
    "\n",
    "for i in range(subset_size):\n",
    "    for j in range(len(jobdes_cleaned[i])):\n",
    "        jobdes_bow_cleaned.append(jobdes_cleaned[i][j])\n",
    "    for k in range(len(jobdes_lemmatized[i])):\n",
    "        jobdes_bow_lemmatized.append(jobdes_lemmatized[i][k])\n",
    "    for l in range(len(jobdes_nostopwords[i])):\n",
    "        jobdes_bow_nostopwords.append(jobdes_nostopwords[i][l])\n",
    "\n",
    "jobdes_bow_cleaned_freq = nltk.FreqDist(jobdes_bow_cleaned)\n",
    "jobdes_bow_lemmatized_freq = nltk.FreqDist(jobdes_bow_lemmatized)\n",
    "jobdes_bow_nostopwords_freq = nltk.FreqDist(jobdes_bow_nostopwords)\n",
    "\n",
    "# print out the most common 50 words with their count\n",
    "for word, frequency in jobdes_bow_cleaned_freq.most_common(10):\n",
    "    print('%s: %d' % (word, frequency)).encode('utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### B1 - Create a classification model with all words and the bag-of-words approach. How accurate is the model (show the confusion matrix)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Classifier accuracy percent:', 78.68)\n",
      "            |             H |\n",
      "            |      L      i |\n",
      "            |      o      g |\n",
      "            |      w      h |\n",
      "            |               |\n",
      "            |      S      S |\n",
      "            |      a      a |\n",
      "            |      l      l |\n",
      "            |      a      a |\n",
      "            |      r      r |\n",
      "            |      y      y |\n",
      "------------+---------------+\n",
      " Low Salary | <68.8%> 12.4% |\n",
      "High Salary |   8.9%  <9.9%>|\n",
      "------------+---------------+\n",
      "(row = reference; col = test)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "document = []\n",
    "\n",
    "# pick a small subset of the dataset\n",
    "for i in range(subset_size):\n",
    "    tup = (nltk.word_tokenize(descriptions[i]), categories[i])\n",
    "    document.append(tup)\n",
    "\n",
    "# we have to shuffle the documents to test on seperate data that did not train again.\n",
    "random.shuffle(document)\n",
    "\n",
    "# reusing descriptions_subset, jobdes_bow, jobdes_word_freq\n",
    "\n",
    "# choose words having the highest frequency among all words as our features, the number of feature = data_size/2\n",
    "jobdes_bow_freq = nltk.FreqDist(jobdes_bow)\n",
    "word_feature = list(jobdes_bow_freq.keys())[:feature_set]\n",
    "\n",
    "# This function will return whether or not the word in feature set appear among the words in document\n",
    "def find_features(document):\n",
    "    words = set(document)\n",
    "    features = {}\n",
    "    for w in word_feature:\n",
    "        features[w] = (w in words) # return boolean if the 500 words feature appear in the document or not\n",
    "\n",
    "    return features\n",
    "\n",
    "# find_feature(description) gives whether or not top 500 words appear in each job description \n",
    "featuresets = [(find_features(description), category) for (description, category) in document]\n",
    "\n",
    "# set that we'll train our classifier with\n",
    "subset_size_mid = subset_size/2\n",
    "training_set = featuresets[:subset_size_mid]\n",
    "# set that we'll test against.\n",
    "testing_set = featuresets[subset_size_mid:]\n",
    "testing_set_mod = [x[0] for x in testing_set]\n",
    "\n",
    "# Running Naive Bayes classifier\n",
    "classifier = nltk.NaiveBayesClassifier.train(training_set)\n",
    "print(\"Classifier accuracy percent:\",(nltk.classify.accuracy(classifier, testing_set))*100)\n",
    "predicted_categories_NaiveBayes =  classifier.classify_many(testing_set_mod)\n",
    "actual_categories = [x[1] for x in testing_set]\n",
    "\n",
    "# Printing confusion matrix\n",
    "cm = nltk.ConfusionMatrix(actual_categories, predicted_categories_NaiveBayes)\n",
    "print(cm.pretty_format(sort_by_count=True, show_percents=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_The model does well on predicting Low Salary correctly (~85% Precision) and much worse on predicting High Salary (~45% Precision) _"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### B2 - Speculate before running the following analysis whether lemmatization would help improve the accuracy of classification. Now create a classification model after lemmatization. Did the classification accuracy increase relative to B1? Comment on your speculation versus the actual results you obtained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Classifier accuracy percent:', 79.2)\n",
      "            |             H |\n",
      "            |      L      i |\n",
      "            |      o      g |\n",
      "            |      w      h |\n",
      "            |               |\n",
      "            |      S      S |\n",
      "            |      a      a |\n",
      "            |      l      l |\n",
      "            |      a      a |\n",
      "            |      r      r |\n",
      "            |      y      y |\n",
      "------------+---------------+\n",
      " Low Salary | <69.7%> 12.0% |\n",
      "High Salary |   8.8%  <9.5%>|\n",
      "------------+---------------+\n",
      "(row = reference; col = test)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "document_lemmatized = []\n",
    "\n",
    "# reusing descriptions_subset, jobdes_bow, jobdes_word_freq, jobdes_bow_lemmatized, jobdes_bow_nostopwords,\n",
    "# jobdes_bow_lemmatized_freq, jobdes_bow_nostopwords_freq, function find_features\n",
    "\n",
    "# pick a small subset of the dataset\n",
    "for i in range(subset_size):\n",
    "    word_tokenize = nltk.word_tokenize(descriptions[i])\n",
    "    lemma_word = map(lambda x: lemmatizer.lemmatize(x), word_tokenize)\n",
    "    tup = (lemma_word, categories[i])\n",
    "    document_lemmatized.append(tup)\n",
    "    \n",
    "random.shuffle(document_lemmatized)\n",
    "\n",
    "jobdes_bow_lemmatized_freq = nltk.FreqDist(jobdes_bow_lemmatized)\n",
    "# choose the top 500 words having the highest frequency among all words as our features\n",
    "word_feature = list(jobdes_bow_lemmatized_freq.keys())[:feature_set]\n",
    "\n",
    "# find_feature(description) gives whether or not top 500 words appear in each job description \n",
    "featuresets = [(find_features(description), category) for (description, category) in document_lemmatized]\n",
    "\n",
    "# set that we'll train our classifier with\n",
    "subset_size_mid = subset_size/2\n",
    "training_set = featuresets[:subset_size_mid]\n",
    "# set that we'll test against.\n",
    "testing_set = featuresets[subset_size_mid:]\n",
    "testing_set_mod = [x[0] for x in testing_set]\n",
    "\n",
    "# Running Naive Bayes classifier\n",
    "classifier = nltk.NaiveBayesClassifier.train(training_set)\n",
    "print(\"Classifier accuracy percent:\",(nltk.classify.accuracy(classifier, testing_set))*100)\n",
    "predicted_categories_NaiveBayes =  classifier.classify_many(testing_set_mod)\n",
    "actual_categories = [x[1] for x in testing_set]\n",
    "\n",
    "# Printing confusion matrix\n",
    "cm = nltk.ConfusionMatrix(actual_categories, predicted_categories_NaiveBayes)\n",
    "print(cm.pretty_format(sort_by_count=True, show_percents=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_The accuracy overall increases to 79.2 from 78.7: The in-class precision also increase in the relative magnitude_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### B3 - Now speculate whether stopwords removal from the original data would help increase the accuracy of the model. Take out the stopwords (but do not lemmatize), build a classification model and check the accuracy, and compare with that in B1 & B2. Also show the top 10 words (excluding stopwords) that are most indicative of (i) high salary, and (ii) low salary.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Classifier accuracy percent:', 78.18)\n",
      "            |             H |\n",
      "            |      L      i |\n",
      "            |      o      g |\n",
      "            |      w      h |\n",
      "            |               |\n",
      "            |      S      S |\n",
      "            |      a      a |\n",
      "            |      l      l |\n",
      "            |      a      a |\n",
      "            |      r      r |\n",
      "            |      y      y |\n",
      "------------+---------------+\n",
      " Low Salary | <68.6%> 12.7% |\n",
      "High Salary |   9.1%  <9.6%>|\n",
      "------------+---------------+\n",
      "(row = reference; col = test)\n",
      "\n",
      "Most Informative Features\n",
      "                 teacher = True           Low Sa : High S =     21.0 : 1.0\n",
      "                    rota = True           Low Sa : High S =     19.1 : 1.0\n",
      "                 exploit = True           High S : Low Sa =     15.5 : 1.0\n",
      "             immigration = True           Low Sa : High S =     13.7 : 1.0\n",
      "                 cooking = True           Low Sa : High S =     13.5 : 1.0\n",
      "                     ibm = True           High S : Low Sa =     12.6 : 1.0\n",
      "                     mba = True           High S : Low Sa =     12.6 : 1.0\n",
      "                 domains = True           High S : Low Sa =     12.6 : 1.0\n",
      "                 j****ee = True           High S : Low Sa =     12.6 : 1.0\n",
      "                realtime = True           High S : Low Sa =     11.7 : 1.0\n",
      "                 michael = True           High S : Low Sa =     11.2 : 1.0\n",
      "                    star = True           Low Sa : High S =     11.0 : 1.0\n",
      "                    sand = True           High S : Low Sa =     11.0 : 1.0\n",
      "               obviously = True           High S : Low Sa =     11.0 : 1.0\n",
      "               oversight = True           High S : Low Sa =     11.0 : 1.0\n",
      "                   night = True           Low Sa : High S =     10.5 : 1.0\n",
      "               infirmity = True           High S : Low Sa =      9.8 : 1.0\n",
      "           architectural = True           High S : Low Sa =      9.8 : 1.0\n",
      "                  embark = True           High S : Low Sa =      9.8 : 1.0\n",
      "                  abaqus = True           High S : Low Sa =      9.8 : 1.0\n",
      "               millshill = True           High S : Low Sa =      9.8 : 1.0\n",
      "                contents = True           High S : Low Sa =      9.8 : 1.0\n",
      "                   quinn = True           High S : Low Sa =      9.8 : 1.0\n",
      "            dependencies = True           High S : Low Sa =      9.3 : 1.0\n",
      "               actuarial = True           High S : Low Sa =      9.3 : 1.0\n",
      "                 ****kpa = True           High S : Low Sa =      9.3 : 1.0\n",
      "              challenged = True           High S : Low Sa =      9.0 : 1.0\n",
      "              initiation = True           High S : Low Sa =      8.0 : 1.0\n",
      "          www.huxley.com = True           High S : Low Sa =      8.0 : 1.0\n",
      "              definition = True           High S : Low Sa =      7.9 : 1.0\n",
      "                assesses = True           High S : Low Sa =      7.8 : 1.0\n",
      "                  spoken = True           Low Sa : High S =      7.7 : 1.0\n",
      "                     ceo = True           High S : Low Sa =      7.6 : 1.0\n",
      "                  fabric = True           High S : Low Sa =      7.6 : 1.0\n",
      "          petrochemicals = True           High S : Low Sa =      7.6 : 1.0\n",
      "                  netapp = True           High S : Low Sa =      7.6 : 1.0\n",
      "                     cmc = True           High S : Low Sa =      7.5 : 1.0\n",
      "                 kitchen = True           Low Sa : High S =      7.4 : 1.0\n",
      "                entities = True           High S : Low Sa =      7.0 : 1.0\n",
      "                adaptive = True           High S : Low Sa =      7.0 : 1.0\n",
      "            anticipating = True           High S : Low Sa =      7.0 : 1.0\n",
      "                ondemand = True           High S : Low Sa =      7.0 : 1.0\n",
      "                toolkits = True           High S : Low Sa =      7.0 : 1.0\n",
      "                powering = True           High S : Low Sa =      7.0 : 1.0\n",
      "                 lan/wan = True           High S : Low Sa =      7.0 : 1.0\n",
      "                    rina = True           High S : Low Sa =      7.0 : 1.0\n",
      "                fortinet = True           High S : Low Sa =      7.0 : 1.0\n",
      "                    dave = True           High S : Low Sa =      7.0 : 1.0\n",
      "                    ppap = True           High S : Low Sa =      7.0 : 1.0\n",
      "                  6month = True           High S : Low Sa =      7.0 : 1.0\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "stop_words = set(stopwords.words(\"english\"))  # load stopwords\n",
    "stop_words.update(['.', ',', '\"', \"'\", '?', '!', ':', ';', '(', ')', '[', ']', '{', '}','****']) \n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "document_nostopwords = []\n",
    "\n",
    "# reusing descriptions_subset, jobdes_bow, jobdes_word_freq, jobdes_bow_lemmatized, jobdes_bow_nostopwords,\n",
    "# jobdes_bow_lemmatized_freq, jobdes_bow_nostopwords_freq, function find_features\n",
    "\n",
    "# pick a small subset of the dataset\n",
    "for i in range(subset_size):\n",
    "    word_tokenize = nltk.word_tokenize(descriptions[i])\n",
    "    cleaned_text = filter(lambda x: x not in stop_words, word_tokenize)\n",
    "    tup = (cleaned_text, categories[i])\n",
    "    document_nostopwords.append(tup)\n",
    "    \n",
    "random.shuffle(document_nostopwords)\n",
    "\n",
    "jobdes_bow_nostopwords_freq = nltk.FreqDist(jobdes_bow_nostopwords)\n",
    "\n",
    "# choose the top 500 words having the highest frequency among all words as our features\n",
    "word_feature = list(jobdes_bow_nostopwords_freq.keys())[:feature_set]\n",
    "\n",
    "# find_feature(description) gives whether or not top 500 words appear in each job description \n",
    "featuresets = [(find_features(description), category) for (description, category) in document_nostopwords]\n",
    "\n",
    "# set that we'll train our classifier with\n",
    "subset_size_mid = subset_size/2\n",
    "training_set = featuresets[:subset_size_mid]\n",
    "# set that we'll test against.\n",
    "testing_set = featuresets[subset_size_mid:]\n",
    "testing_set_mod = [x[0] for x in testing_set]\n",
    "\n",
    "# Running Naive Bayes classifier\n",
    "classifier = nltk.NaiveBayesClassifier.train(training_set)\n",
    "print(\"Classifier accuracy percent:\",(nltk.classify.accuracy(classifier, testing_set))*100)\n",
    "predicted_categories_NaiveBayes =  classifier.classify_many(testing_set_mod)\n",
    "actual_categories = [x[1] for x in testing_set]\n",
    "\n",
    "# Printing confusion matrix\n",
    "cm = nltk.ConfusionMatrix(actual_categories, predicted_categories_NaiveBayes)\n",
    "print(cm.pretty_format(sort_by_count=True, show_percents=True))\n",
    "\n",
    "# Printing indicative words for high salary and low salary\n",
    "print classifier.show_most_informative_features(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_The overall accuracy remains around the same as B2_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### B4 - Use the job descriptions without lemmatiztion and stopword removal. Add parts-of-speech bigrams to the bag-of-words, and run a new classification model. Does the accuracy increase over the results in B1?  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Classifier accuracy percent:', 77.29545909181836)\n",
      "            |             H |\n",
      "            |      L      i |\n",
      "            |      o      g |\n",
      "            |      w      h |\n",
      "            |               |\n",
      "            |      S      S |\n",
      "            |      a      a |\n",
      "            |      l      l |\n",
      "            |      a      a |\n",
      "            |      r      r |\n",
      "            |      y      y |\n",
      "------------+---------------+\n",
      " Low Salary | <67.0%> 13.6% |\n",
      "High Salary |   9.1% <10.3%>|\n",
      "------------+---------------+\n",
      "(row = reference; col = test)\n",
      "\n",
      "Most Informative Features\n",
      "              initiation = True           High S : Low Sa =     30.9 : 1.0\n",
      "                 j****ee = True           High S : Low Sa =     22.1 : 1.0\n",
      "           (\"''\", 'NNS') = True           High S : Low Sa =     19.2 : 1.0\n",
      "                     ibm = True           High S : Low Sa =     19.2 : 1.0\n",
      "             integrating = True           High S : Low Sa =     18.6 : 1.0\n",
      "                    rota = True           Low Sa : High S =     17.1 : 1.0\n",
      "                 teacher = True           Low Sa : High S =     17.0 : 1.0\n",
      "          www.huxley.com = True           High S : Low Sa =     16.8 : 1.0\n",
      "               caseloads = True           High S : Low Sa =     16.2 : 1.0\n",
      "               increment = True           High S : Low Sa =     16.2 : 1.0\n",
      "                 fortune = True           High S : Low Sa =     16.2 : 1.0\n",
      "          ('VBN', 'RBR') = True           High S : Low Sa =     13.3 : 1.0\n",
      "                 exploit = True           High S : Low Sa =     13.3 : 1.0\n",
      "            ('NNP', ':') = True           High S : Low Sa =     13.3 : 1.0\n",
      "               millshill = True           High S : Low Sa =     13.3 : 1.0\n",
      "                   istqb = True           High S : Low Sa =     13.3 : 1.0\n",
      "             ('$', 'IN') = True           High S : Low Sa =     13.3 : 1.0\n",
      "                  macros = True           High S : Low Sa =     13.3 : 1.0\n",
      "                     aml = True           High S : Low Sa =     13.3 : 1.0\n",
      "                assesses = True           High S : Low Sa =     13.3 : 1.0\n",
      "            satisfactory = True           Low Sa : High S =     12.9 : 1.0\n",
      "                weekends = True           Low Sa : High S =     12.6 : 1.0\n",
      "              testdriven = True           High S : Low Sa =     10.3 : 1.0\n",
      "                  majors = True           High S : Low Sa =     10.3 : 1.0\n",
      "        highavailability = True           High S : Low Sa =     10.3 : 1.0\n",
      "           environmentto = True           High S : Low Sa =     10.3 : 1.0\n",
      "               expansive = True           High S : Low Sa =     10.3 : 1.0\n",
      "          ('VBD', 'JJS') = True           High S : Low Sa =     10.3 : 1.0\n",
      "            ('(', 'WDT') = True           High S : Low Sa =      9.7 : 1.0\n",
      "              visibility = True           High S : Low Sa =      9.7 : 1.0\n",
      "                seasoned = True           High S : Low Sa =      9.7 : 1.0\n",
      "                   night = True           Low Sa : High S =      9.6 : 1.0\n",
      "             immigration = True           Low Sa : High S =      9.2 : 1.0\n",
      "             (',', 'EX') = True           Low Sa : High S =      9.0 : 1.0\n",
      "         implementations = True           High S : Low Sa =      8.5 : 1.0\n",
      "                    ssis = True           High S : Low Sa =      7.8 : 1.0\n",
      "           architectural = True           High S : Low Sa =      7.6 : 1.0\n",
      "             appreciated = True           High S : Low Sa =      7.4 : 1.0\n",
      "                     20m = True           High S : Low Sa =      7.4 : 1.0\n",
      "                   grave = True           High S : Low Sa =      7.4 : 1.0\n",
      "                 schemas = True           High S : Low Sa =      7.4 : 1.0\n",
      "          ('VBD', 'PDT') = True           High S : Low Sa =      7.4 : 1.0\n",
      "                     brc = True           High S : Low Sa =      7.4 : 1.0\n",
      "            ('WDT', ',') = True           High S : Low Sa =      7.4 : 1.0\n",
      "            ('RP', 'MD') = True           High S : Low Sa =      7.4 : 1.0\n",
      "           accomplishing = True           High S : Low Sa =      7.4 : 1.0\n",
      "             multiagency = True           High S : Low Sa =      7.4 : 1.0\n",
      "                    swan = True           High S : Low Sa =      7.4 : 1.0\n",
      "                  abaqus = True           High S : Low Sa =      7.4 : 1.0\n",
      "              permanency = True           High S : Low Sa =      7.4 : 1.0\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "document_pos_bigrams = []\n",
    "\n",
    "# pick a small subset of the dataset\n",
    "for i in range(subset_size):\n",
    "    word_tokenize = nltk.word_tokenize(descriptions[i])\n",
    "    word_tokenize_pos = nltk.pos_tag(word_tokenize)\n",
    "    pos = [x[1] for x in word_tokenize_pos]\n",
    "    pos_bigram = nltk.ngrams(pos,2)\n",
    "    tup = (word_tokenize + list(pos_bigram), categories[i])\n",
    "    document_pos_bigrams.append(tup)\n",
    "    \n",
    "random.shuffle(document_pos_bigrams)\n",
    "\n",
    "# reusing descriptions_subset, jobdes_bow, jobdes_word_freq\n",
    "\n",
    "# choose the top 500 pos-bigrams having the highest frequency among all words as our features\n",
    "jobdes_pos = nltk.pos_tag(jobdes_bow)\n",
    "jobdes_pos_only = [x[1] for x in jobdes_pos]\n",
    "jobdes_pos_bigrams =  nltk.ngrams(jobdes_pos_only,2)\n",
    "# print document_pos_bigrams[:10]\n",
    "jobdes_pos_bigrams_freq = nltk.FreqDist(jobdes_pos_bigrams)\n",
    "word_feature_1 = list(jobdes_pos_bigrams_freq.keys())[:feature_set]\n",
    "word_feature_2 = list(jobdes_bow_freq.keys())[:feature_set]\n",
    "word_feature = word_feature_2 + word_feature_1\n",
    "# document_pos_bigrams = document + document_pos_bigrams\n",
    "\n",
    "def find_bigram_features(document_pos_bigrams):\n",
    "    words = set(document_pos_bigrams)\n",
    "    features = {}\n",
    "#     count = 0\n",
    "    for w_bigram in word_feature:\n",
    "        features[w_bigram] = (w_bigram in words) # return boolean if the 500 words feature appear in the document or not\n",
    "#         for ww in words:\n",
    "#             if w == ww:\n",
    "#                 count += 1\n",
    "#         features[w] = (count) # return count the each 50 words feature appear in the document\n",
    "    return features\n",
    "\n",
    "# find_feature(description) gives whether or not top 500 words appear in each job description \n",
    "featuresets = [(find_bigram_features(description), category) for (description, category) in document_pos_bigrams]\n",
    "\n",
    "# set that we'll train our classifier with\n",
    "subset_size_mid = subset_size/2\n",
    "training_set = featuresets[:subset_size_mid]\n",
    "# set that we'll test against.\n",
    "testing_set = featuresets[subset_size_mid+1:]\n",
    "testing_set_mod = [x[0] for x in testing_set]\n",
    "\n",
    "# Running Naive Bayes classifier\n",
    "classifier = nltk.NaiveBayesClassifier.train(training_set)\n",
    "print(\"Classifier accuracy percent:\",(nltk.classify.accuracy(classifier, testing_set))*100)\n",
    "predicted_categories_NaiveBayes =  classifier.classify_many(testing_set_mod)\n",
    "actual_categories = [x[1] for x in testing_set]\n",
    "\n",
    "# Printing confusion matrix\n",
    "cm = nltk.ConfusionMatrix(actual_categories, predicted_categories_NaiveBayes)\n",
    "print(cm.pretty_format(sort_by_count=True, show_percents=True))\n",
    "\n",
    "# Printing indicative words for high salary and low salary\n",
    "print classifier.show_most_informative_features(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_The overall performance of the model does not change significantly_"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
