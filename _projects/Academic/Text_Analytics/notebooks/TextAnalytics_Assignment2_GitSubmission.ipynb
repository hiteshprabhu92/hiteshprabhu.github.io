{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Analytics\n",
    "\n",
    "\n",
    "# Assignment 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Submitted by:*  \n",
    "\n",
    "**Zena Drakou | Marissa Hausman | Hitesh Prabhu | Chase Slocum | Yawen Ye - MSBA 2017 **\n",
    "\n",
    "**Sept 21, 2016**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initial Setup:\n",
    "\n",
    "1. Importing libraries\n",
    "2. Reading dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas import Series, DataFrame\n",
    "\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "from sklearn import metrics\n",
    "\n",
    "import matplotlib.pyplot as plt \n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>stars</th>\n",
       "      <th>votes_cool</th>\n",
       "      <th>votes_funny</th>\n",
       "      <th>votes_useful</th>\n",
       "      <th>Cheap</th>\n",
       "      <th>Moderate</th>\n",
       "      <th>Expensive</th>\n",
       "      <th>VeryExpensive</th>\n",
       "      <th>American</th>\n",
       "      <th>Chinese</th>\n",
       "      <th>...</th>\n",
       "      <th>Indian</th>\n",
       "      <th>Italian</th>\n",
       "      <th>Greek</th>\n",
       "      <th>Mediterranean</th>\n",
       "      <th>Mexican</th>\n",
       "      <th>Thai</th>\n",
       "      <th>Vietnamese</th>\n",
       "      <th>Others</th>\n",
       "      <th>Review</th>\n",
       "      <th>rating_class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>19990</th>\n",
       "      <td>5</td>\n",
       "      <td>9</td>\n",
       "      <td>3</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Well  hate to join the bandwagon  but this pla...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19991</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Tucked into a small stripmall  this tiny sushi...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19992</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>I have been looking for great sushi in AZ and ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19993</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>A very small local restaurant with a very inti...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19994</th>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>my first yelp! review  and it s because of Yas...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19995</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Good.  Intimate.  Expensive.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19996</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Great  place! The food is really good. This is...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19997</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>The location is convenient and I would say tha...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19998</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>I am a sushi snob.I live in Chicago  but used ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       stars  votes_cool  votes_funny  votes_useful  Cheap  Moderate  \\\n",
       "19990      5           9            3            13      0         0   \n",
       "19991      3           1            1             1      0         0   \n",
       "19992      5           0            0             0      0         0   \n",
       "19993      4           0            0             2      0         0   \n",
       "19994      5           3            1             4      0         0   \n",
       "19995      3           0            0             0      0         0   \n",
       "19996      5           0            0             0      0         0   \n",
       "19997      3           0            1             0      0         0   \n",
       "19998      5           0            0             0      0         0   \n",
       "\n",
       "       Expensive  VeryExpensive  American  Chinese      ...       Indian  \\\n",
       "19990          1              0         0        0      ...            0   \n",
       "19991          1              0         0        0      ...            0   \n",
       "19992          1              0         0        0      ...            0   \n",
       "19993          1              0         0        0      ...            0   \n",
       "19994          1              0         0        0      ...            0   \n",
       "19995          1              0         0        0      ...            0   \n",
       "19996          1              0         0        0      ...            0   \n",
       "19997          1              0         0        0      ...            0   \n",
       "19998          1              0         0        0      ...            0   \n",
       "\n",
       "       Italian  Greek  Mediterranean  Mexican  Thai  Vietnamese  Others  \\\n",
       "19990        0      0              0        0     0           0       1   \n",
       "19991        0      0              0        0     0           0       1   \n",
       "19992        0      0              0        0     0           0       1   \n",
       "19993        0      0              0        0     0           0       1   \n",
       "19994        0      0              0        0     0           0       1   \n",
       "19995        0      0              0        0     0           0       1   \n",
       "19996        0      0              0        0     0           0       1   \n",
       "19997        0      0              0        0     0           0       1   \n",
       "19998        0      0              0        0     0           0       1   \n",
       "\n",
       "                                                  Review  rating_class  \n",
       "19990  Well  hate to join the bandwagon  but this pla...             1  \n",
       "19991  Tucked into a small stripmall  this tiny sushi...             0  \n",
       "19992  I have been looking for great sushi in AZ and ...             1  \n",
       "19993  A very small local restaurant with a very inti...             1  \n",
       "19994  my first yelp! review  and it s because of Yas...             1  \n",
       "19995                       Good.  Intimate.  Expensive.             0  \n",
       "19996  Great  place! The food is really good. This is...             1  \n",
       "19997  The location is convenient and I would say tha...             0  \n",
       "19998  I am a sushi snob.I live in Chicago  but used ...             1  \n",
       "\n",
       "[9 rows x 22 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yelp = pd.read_csv('../files/Yelp Data Restaurant Reviews Ratings.csv')\n",
    "yelp[\"rating_class\"] = 0\n",
    "yelp.loc[yelp[\"stars\"] >= 4, \"rating_class\"] = 1\n",
    "yelp.loc[yelp[\"stars\"] <= 3, \"rating_class\"] = 0\n",
    "yelp[19990:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ignore the text (reviews) and run a classification model with the numeric data (you can use standard methods like logistic regression, k-nearest neighbors or anything else). What is the best accuracy of your model with numeric data?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**KNN**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda2\\envs\\hitesh27\\lib\\site-packages\\ipykernel_launcher.py:50: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 \t0.610202550638\n",
      "5 \t0.63375843961\n",
      "7 \t0.635858964741\n",
      "9 \t0.654313578395\n",
      "12 \t0.637059264816\n",
      "16 \t0.675618904726\n",
      "20 \t0.677269317329\n",
      "25 \t0.681770442611\n",
      "35 \t0.686271567892\n",
      "50 \t0.688672168042\n",
      "75 \t0.687321830458\n",
      "100 \t0.685821455364\n",
      "250 \t0.681320330083\n",
      "500 \t0.678019504876\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYwAAAEKCAYAAAAB0GKPAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nhYvBESY/C58AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0xea881d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import random\n",
    "from numpy.random import permutation\n",
    "yelp_num = yelp\n",
    "yelp_num = yelp_num.drop('Review', 1)\n",
    "\n",
    "# Randomly shuffle the index of yelp_num.\n",
    "random_index = permutation(yelp_num.index)\n",
    "\n",
    "# Set a cutoff for how many items we want in the test set \n",
    "cutoff = math.floor(len(yelp_num)/3)\n",
    "cutoff = int(cutoff)\n",
    "\n",
    "# Generate the test set by taking the first 1/3 of the randomly shuffled index.\n",
    "test = yelp_num.loc[random_index[1:cutoff]]\n",
    "\n",
    "# Generate the train set.\n",
    "train = yelp_num.loc[random_index[cutoff:]]\n",
    "\n",
    "# The columns that we will be making predictions with.\n",
    "x_columns = ['votes_cool',\n",
    "             'votes_funny',\n",
    "             'votes_useful',\n",
    "             'Cheap',\n",
    "             'Moderate',\n",
    "             'Expensive',\n",
    "             'VeryExpensive',\n",
    "             'American',\n",
    "             'Chinese',\n",
    "             'French',\n",
    "             'Japanese',\n",
    "             'Indian',\n",
    "             'Italian',\n",
    "             'Greek',\n",
    "             'Mediterranean',\n",
    "             'Mexican',\n",
    "             'Thai',\n",
    "             'Vietnamese',\n",
    "             'Others']\n",
    "\n",
    "# The column that we want to predict.\n",
    "y_column = [\"rating_class\"]\n",
    "\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# Function runs Knn for given value of k\n",
    "def run_knn(n):\n",
    "    model = KNeighborsClassifier(n_neighbors = n,\n",
    "                                 weights = 'uniform')\n",
    "    model.fit(train[x_columns], train[y_column].values)\n",
    "\n",
    "    # Make predictions on the test set using the fitted model.\n",
    "    predictions = model.predict(test[x_columns].values)\n",
    "    accuracy = metrics.accuracy_score(test[y_column], predictions)\n",
    "    return accuracy\n",
    "\n",
    "k_list = [3,5,7,9,12,16,20,25,35,50,75,100,250,500]  \n",
    "scores = [run_knn(k) for k in k_list]\n",
    "plt.plot(k_list, scores)\n",
    "plt.xlabel('Number of nearest neighbors')\n",
    "plt.ylabel('Accuracy of classifier')\n",
    "\n",
    "for k, scores in zip(k_list, scores):\n",
    "    print k, \"\\t\", scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda2\\envs\\hitesh27\\lib\\site-packages\\ipykernel_launcher.py:5: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  \"\"\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN run for n = 50\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.60      0.10      0.17      2146\n",
      "          1       0.69      0.97      0.81      4519\n",
      "\n",
      "avg / total       0.66      0.69      0.60      6665\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Looks like k=50 looks like a good value for k\n",
    "k = 50\n",
    "model = KNeighborsClassifier(n_neighbors = k,\n",
    "                                 weights = 'uniform')\n",
    "model.fit(train[x_columns], train[y_column])\n",
    "\n",
    "# Make predictions on the test set using the fitted model.\n",
    "predictions = model.predict(test[x_columns])\n",
    "accuracy = metrics.accuracy_score(test[y_column], predictions)\n",
    "print \"KNN run for n = {}\".format(k)\n",
    "print(metrics.classification_report(test[y_column], predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_The accuracy obtained for knn was 67%._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Logistic Regression**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.63      0.06      0.11      2146\n",
      "          1       0.69      0.98      0.81      4519\n",
      "\n",
      "avg / total       0.67      0.69      0.58      6665\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda2\\envs\\hitesh27\\lib\\site-packages\\sklearn\\utils\\validation.py:547: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "logistic_model = LogisticRegression()\n",
    "logistic_model.fit(train[x_columns], train[y_column])\n",
    "\n",
    "y = test[y_column]\n",
    "predict_logistic = logistic_model.predict(test[x_columns])\n",
    "\n",
    "#print metrics.accuracy_score(y, predict_logistic)\n",
    "print(metrics.classification_report(y, predict_logistic))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_The accuracy obtained for Logistic Regression was 66%._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SVC**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.66      0.03      0.05      2146\n",
      "          1       0.68      0.99      0.81      4519\n",
      "\n",
      "avg / total       0.67      0.68      0.56      6665\n",
      "\n",
      "[[  54 2092]\n",
      " [  28 4491]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "model2 = SVC()\n",
    "model2.fit(train[x_columns], train[y_column])\n",
    "\n",
    "# make predictions\n",
    "expected = test[y_column]\n",
    "predicted = model2.predict(test[x_columns])\n",
    "\n",
    "# summarize the fit of the model\n",
    "print(metrics.classification_report(expected, predicted))\n",
    "print(metrics.confusion_matrix(expected, predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_The accuracy obtained for SVM model was 68%._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline accuracy = 0.677683884194\n"
     ]
    }
   ],
   "source": [
    "# Baseline score\n",
    "len(yelp[yelp['rating_class'] == 1])\n",
    "len(yelp[yelp['rating_class'] == 0])\n",
    "\n",
    "print \"Baseline accuracy =\", float(len(yelp[yelp['rating_class'] == 1]))/( float(len(yelp[yelp['rating_class'] == 1])) + float(len(yelp[yelp['rating_class'] == 0])))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "All three classification models that we tested on the with the numeric data gave a similar accuracy of around 66-68%, which doesn't really improve upon the baseline accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform a supervised classification on a subset of the corpus using the reviews only. You can write your code in Python or R. What accuracy do you get from this text mining exercise?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.84      0.67      0.74      3194\n",
      "          1       0.86      0.94      0.90      6805\n",
      "\n",
      "avg / total       0.85      0.85      0.85      9999\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# put rating class into list\n",
    "rating_class = yelp['rating_class'].values.tolist()\n",
    "\n",
    "corpus = yelp['Review']\n",
    "corpus_list = corpus.values.tolist()\n",
    "\n",
    "bag_of_words_vectorizer = CountVectorizer(min_df = 1)\n",
    "matrix_corpus = bag_of_words_vectorizer.fit_transform(corpus_list)\n",
    "words_counts = matrix_corpus.toarray()\n",
    "\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "tfidf = tfidf_transformer.fit_transform(words_counts)\n",
    "\n",
    "bag_of_words_vectorizer.get_feature_names()\n",
    "text = tfidf.toarray()\n",
    "\n",
    "text_train = text[:10000]\n",
    "text_test = text[10000:]\n",
    "\n",
    "text_aim1 = []\n",
    "text_aim2 = []\n",
    "\n",
    "for i in range(10000):\n",
    "    text_aim1.append(rating_class[i])\n",
    "\n",
    "for i in range(10000,19999):\n",
    "    text_aim2.append(rating_class[i])\n",
    "\n",
    "\n",
    "logistic_model3 = LogisticRegression()\n",
    "logistic_model3.fit(text_train, text_aim1)\n",
    "\n",
    "predict_logistic3 = logistic_model3.predict(text_test)\n",
    "#print metrics.accuracy_score(y, predict_logistic)\n",
    "print(metrics.classification_report(text_aim2, predict_logistic3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8533"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# See how many 'matched' pair between predicted and actual rating value. They are in total 9999 pairs\n",
    "count = 0\n",
    "for i in range(len(text_test)):\n",
    "    if text_aim2[i] == predict_logistic3[i]:\n",
    "        count = count + 1\n",
    "count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_In the test set of 9999 rows between predicted and actual value, there are 8543 pairs which are matched (predicted and actual values are equal). So the percent of accuracy is 8543/9999 = 85%._\n",
    "\n",
    "_Comment: As we can see, by only using the text part (reviews) to train the logistic model and prediction, we can achieve a test accuracy about 85%._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TASK C"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine the numeric data and the text classification model (in task B) to create a “hybrid” model. It is your task to figure out how to do this. Now run this hybrid classification model and compare the results with those in A and B. Does the numeric data add to the predictive power relative to text?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "import numpy as np\n",
    "\n",
    "# extract only numeric column from table\n",
    "numeric = yelp[x_columns]\n",
    "# put each row of numeric into list\n",
    "numeric_list = numeric.values.tolist()\n",
    "# put rating class into list\n",
    "rating_class = yelp['rating_class'].values.tolist()\n",
    "\n",
    "corpus = yelp['Review']\n",
    "corpus_list = corpus.map(lambda s: s.decode('latin-1').encode(\"utf-8\")).tolist()\n",
    "bag_of_words_vectorizer = CountVectorizer(min_df=1)\n",
    "matrix_corpus = bag_of_words_vectorizer.fit_transform(corpus_list)\n",
    "words_counts = matrix_corpus.toarray()\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "tfidf = tfidf_transformer.fit_transform(words_counts)\n",
    "bag_of_words_vectorizer.get_feature_names()\n",
    "\n",
    "# Combine the numeric features with the text features by stacking them\n",
    "combinedFeatures = np.hstack([numeric_list, tfidf.toarray()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.84      0.67      0.74      3194\n",
      "          1       0.86      0.94      0.90      6805\n",
      "\n",
      "avg / total       0.85      0.85      0.85      9999\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train = combinedFeatures[:10000]\n",
    "test = combinedFeatures[10000:]\n",
    "\n",
    "aim1 = []\n",
    "aim2 = []\n",
    "\n",
    "for i in range(10000):\n",
    "    aim1.append(rating_class[i])\n",
    "\n",
    "for i in range(10000,19999):\n",
    "    aim2.append(rating_class[i])\n",
    "\n",
    "\n",
    "logistic_model2 = LogisticRegression()\n",
    "logistic_model2.fit(train, aim1)\n",
    "\n",
    "predict_logistic2 = logistic_model2.predict(test)\n",
    "\n",
    "#print metrics.accuracy_score(y, predict_logistic)\n",
    "print(metrics.classification_report(aim2, predict_logistic2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8529"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# See how many 'matched' pair between predicted and actual rating value. They are in total 9999 pairs\n",
    "\n",
    "test = 0\n",
    "\n",
    "for i in range(len(aim2)):\n",
    "    if aim2[i] == predict_logistic2[i]:\n",
    "        test = test + 1\n",
    "test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_It seems that combining the numeric feature does not add power to our prediction compared to B. In part B, we achieved a percent of accuracy about 85%, but after hybrid the numeric features with the text features, the accuracy stays at the same level, which suggests that the numeric values/features does not contribute to our prediction. The text features seem more relevant for the customers' actual rating._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TASK D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use unsupervised sentiment analysis on the reviews (with SentiStrength or any other tool) and use the sentiment scores to predict high/low rating. Compare and contrast the results of tasks B and D. What can you conclude from your analysis?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Review</th>\n",
       "      <th>Positive</th>\n",
       "      <th>Negative</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>In_Words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CLOSED            This JB s locati...</td>\n",
       "      <td>2</td>\n",
       "      <td>-2</td>\n",
       "      <td>0</td>\n",
       "      <td>nuetral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>This is just a basic (albeit mini) chain greas...</td>\n",
       "      <td>2</td>\n",
       "      <td>-2</td>\n",
       "      <td>0</td>\n",
       "      <td>nuetral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Whenever I offer to take my mom out to lunch s...</td>\n",
       "      <td>4</td>\n",
       "      <td>-4</td>\n",
       "      <td>0</td>\n",
       "      <td>nuetral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>If I say it wasn t as bad as I was expecting i...</td>\n",
       "      <td>3</td>\n",
       "      <td>-3</td>\n",
       "      <td>0</td>\n",
       "      <td>nuetral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I ve always said if the guacamole  chips and s...</td>\n",
       "      <td>3</td>\n",
       "      <td>-3</td>\n",
       "      <td>0</td>\n",
       "      <td>nuetral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Had the signature Black Chile entree.  It was ...</td>\n",
       "      <td>3</td>\n",
       "      <td>-1</td>\n",
       "      <td>2</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>After hitting up the bank to sign some paper w...</td>\n",
       "      <td>4</td>\n",
       "      <td>-4</td>\n",
       "      <td>0</td>\n",
       "      <td>nuetral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Great happy hour deals here! I loved the Cotij...</td>\n",
       "      <td>4</td>\n",
       "      <td>-5</td>\n",
       "      <td>-1</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Fine. Just fine. C /B- average-- all around.  ...</td>\n",
       "      <td>4</td>\n",
       "      <td>-3</td>\n",
       "      <td>1</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>beautiful atmosphere...good prices (for the bi...</td>\n",
       "      <td>3</td>\n",
       "      <td>-1</td>\n",
       "      <td>2</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Review  Positive  Negative  \\\n",
       "0              CLOSED            This JB s locati...         2        -2   \n",
       "1  This is just a basic (albeit mini) chain greas...         2        -2   \n",
       "2  Whenever I offer to take my mom out to lunch s...         4        -4   \n",
       "3  If I say it wasn t as bad as I was expecting i...         3        -3   \n",
       "4  I ve always said if the guacamole  chips and s...         3        -3   \n",
       "5  Had the signature Black Chile entree.  It was ...         3        -1   \n",
       "6  After hitting up the bank to sign some paper w...         4        -4   \n",
       "7  Great happy hour deals here! I loved the Cotij...         4        -5   \n",
       "8  Fine. Just fine. C /B- average-- all around.  ...         4        -3   \n",
       "9  beautiful atmosphere...good prices (for the bi...         3        -1   \n",
       "\n",
       "   Sentiment  In_Words  \n",
       "0          0   nuetral  \n",
       "1          0   nuetral  \n",
       "2          0   nuetral  \n",
       "3          0   nuetral  \n",
       "4          0   nuetral  \n",
       "5          2  positive  \n",
       "6          0   nuetral  \n",
       "7         -1  negative  \n",
       "8          1  positive  \n",
       "9          2  positive  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SentiStrength_Results = pd.read_csv(\"../files/YelpReviews_SentiStrength_Results.csv\")\n",
    "SentiStrength_Results[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   stars  votes_cool  votes_funny  votes_useful  Cheap  Moderate  Expensive  \\\n",
      "1      2           2            2             2      1         0          0   \n",
      "2      4           0            0             1      1         0          0   \n",
      "3      3           0            1             2      1         0          0   \n",
      "4      3           7            9             9      1         0          0   \n",
      "5      2           0            0             1      0         1          0   \n",
      "\n",
      "   VeryExpensive  American  Chinese      ...       Indian  Italian  Greek  \\\n",
      "1              0         0        0      ...            0        0      0   \n",
      "2              0         0        0      ...            0        0      0   \n",
      "3              0         0        0      ...            0        0      0   \n",
      "4              0         0        0      ...            0        0      0   \n",
      "5              0         0        0      ...            0        0      0   \n",
      "\n",
      "   Mediterranean  Mexican  Thai  Vietnamese  Others  \\\n",
      "1              0        0     0           0       1   \n",
      "2              0        0     0           0       1   \n",
      "3              0        0     0           0       1   \n",
      "4              0        0     0           0       1   \n",
      "5              0        1     0           0       0   \n",
      "\n",
      "                                              Review  rating_class  \n",
      "1  = = = = = = CLOSED = = = = = =This JB s locati...             0  \n",
      "2  This is just a basic (albeit mini) chain greas...             1  \n",
      "3  Whenever I offer to take my mom out to lunch s...             0  \n",
      "4  If I say it wasn t as bad as I was expecting i...             0  \n",
      "5  I ve always said if the guacamole  chips and s...             0  \n",
      "\n",
      "[5 rows x 22 columns]\n"
     ]
    }
   ],
   "source": [
    "def classifier(Sentiment):\n",
    "    if Sentiment > 0:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "\n",
    "SentiStrength_Results['Prediction'] = SentiStrength_Results['Sentiment'].apply(classifier)\n",
    "# For some unkown reason, the first review did not get rated on SentiStrength. Some bug with row 2 \n",
    "# ( even though I omitted only row 1)\n",
    "yelp = yelp[yelp['Review'] != \"This location is out of business. I drove by it on my way to Costco and it just has a giant for lease sign.\"]\n",
    "print yelp[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.56      0.57      0.56      6445\n",
      "          1       0.79      0.78      0.79     13553\n",
      "\n",
      "avg / total       0.72      0.71      0.71     19998\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_predicted = SentiStrength_Results['Prediction']\n",
    "y_actual = yelp['rating_class']\n",
    "\n",
    "print(metrics.classification_report(y_actual, y_predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_After using unsupervised learning with SentiStrength, we were able to improve upon numeric-only models A and B with a 72% prediction accuracy - so just a slight imporvement over the baseline accuracy of 67%. Howvever, comparing it to task B, where we used using a supervised logistic regression models on the text provided far higher accuracies (~85%)._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task E"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement the PMI approach to sentiment analysis (in either Python or R), and run the classification model with the sentiment scores. How do your results compare with those in Task D?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#randomly sample 3000 rows\n",
    "yelp_sample=yelp.sample(n=3000, random_state=10)\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "# remove it if you need punctuation\n",
    "stop_words.update(['.', ',', '\"', \"'\", '?', '!', ':', ';', '(', ')', '[', ']',\\\n",
    "                   '{', '}','****', '#', '$', '-', '...', 'u', '&']) \n",
    "\n",
    "review = []\n",
    "def tolist_utf8(h):\n",
    "    review.append(h.decode('utf-8').lower())\n",
    "\n",
    "r = yelp_sample[\"Review\"]\n",
    "r.map(tolist_utf8)\n",
    "\n",
    "review_tokenize = []\n",
    "\n",
    "#tokenize reviews\n",
    "for review in review:\n",
    "    word_tokenize = nltk.word_tokenize(review)\n",
    "    lemma_word = map(lambda x: lemmatizer.lemmatize(x), word_tokenize)\n",
    "    cleaned_text = filter(lambda x: x not in stop_words, lemma_word)\n",
    "    review_tokenize.append(cleaned_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'wa', 7391),\n",
       " (u'food', 2520),\n",
       " (u'good', 2420),\n",
       " (u'place', 2411),\n",
       " (u'great', 1577),\n",
       " (u'like', 1496),\n",
       " (u'time', 1327),\n",
       " (u'one', 1236),\n",
       " (u'go', 1146),\n",
       " (u'get', 1113),\n",
       " (u'service', 1098),\n",
       " (u'really', 1093),\n",
       " (u'restaurant', 1032),\n",
       " (u'would', 1026),\n",
       " (u'back', 996),\n",
       " (u'menu', 843),\n",
       " (u'ordered', 797),\n",
       " (u'chicken', 782),\n",
       " (u'also', 759),\n",
       " (u'love', 740),\n",
       " (u'little', 726),\n",
       " (u'order', 687),\n",
       " (u'best', 668),\n",
       " (u'lunch', 665),\n",
       " (u'got', 659),\n",
       " (u'nice', 648),\n",
       " (u'table', 644),\n",
       " (u'salad', 642),\n",
       " (u'try', 635),\n",
       " (u'always', 613),\n",
       " (u'ha', 609),\n",
       " (u'even', 599),\n",
       " (u'well', 590),\n",
       " (u'make', 584),\n",
       " (u'sauce', 582),\n",
       " (u'drink', 571),\n",
       " (u'thing', 563),\n",
       " (u'meal', 561),\n",
       " (u'delicious', 554),\n",
       " (u'dish', 551),\n",
       " (u'went', 538),\n",
       " (u'much', 535),\n",
       " (u'cheese', 530),\n",
       " (u'burger', 529),\n",
       " (u'pretty', 527),\n",
       " (u'know', 505),\n",
       " (u'come', 495),\n",
       " (u'could', 494),\n",
       " (u'price', 487),\n",
       " (u'think', 480),\n",
       " (u'came', 476),\n",
       " (u'eat', 470),\n",
       " (u'friendly', 465),\n",
       " (u'phoenix', 455),\n",
       " (u'say', 451),\n",
       " (u'dinner', 451),\n",
       " (u'going', 449),\n",
       " (u'friend', 448),\n",
       " (u'people', 446),\n",
       " (u'two', 444),\n",
       " (u'night', 441),\n",
       " (u'never', 440),\n",
       " (u'better', 437),\n",
       " (u'sandwich', 437),\n",
       " (u'taco', 435),\n",
       " (u'way', 433),\n",
       " (u'first', 433),\n",
       " (u'right', 432),\n",
       " (u'bar', 432),\n",
       " (u'mexican', 427),\n",
       " (u'fresh', 426),\n",
       " (u'lot', 426),\n",
       " (u'bit', 423),\n",
       " (u'staff', 410),\n",
       " (u'made', 405),\n",
       " (u'star', 403),\n",
       " (u'hour', 401),\n",
       " (u'want', 397),\n",
       " (u'server', 395),\n",
       " (u'happy', 392),\n",
       " (u'experience', 391),\n",
       " (u'wait', 390),\n",
       " (u'side', 389),\n",
       " (u'tasty', 389),\n",
       " (u'take', 384),\n",
       " (u'fry', 383),\n",
       " (u'special', 383),\n",
       " (u'salsa', 373),\n",
       " (u'flavor', 372),\n",
       " (u'definitely', 366),\n",
       " (u'still', 361),\n",
       " (u'ever', 361),\n",
       " (u'amazing', 361),\n",
       " (u'next', 353),\n",
       " (u'sure', 352),\n",
       " (u'chip', 350),\n",
       " (u'give', 350),\n",
       " (u'something', 349),\n",
       " (u'review', 345),\n",
       " (u'though', 341),\n",
       " (u'wine', 340),\n",
       " (u'rice', 338),\n",
       " (u'pizza', 338),\n",
       " (u'meat', 337),\n",
       " (u'small', 335),\n",
       " (u'day', 335),\n",
       " (u'--', 333),\n",
       " (u'since', 331),\n",
       " (u'favorite', 329),\n",
       " (u'bad', 329),\n",
       " (u'said', 329),\n",
       " (u'last', 328),\n",
       " (u'new', 321),\n",
       " (u'everything', 321),\n",
       " (u'around', 319),\n",
       " (u'taste', 319),\n",
       " (u'plate', 318),\n",
       " (u'year', 314),\n",
       " (u'bean', 311),\n",
       " (u'many', 304),\n",
       " (u'find', 302),\n",
       " (u'sweet', 302),\n",
       " (u'2', 298),\n",
       " (u'atmosphere', 298),\n",
       " (u'area', 297),\n",
       " (u'hot', 296),\n",
       " (u'another', 294),\n",
       " (u'every', 294),\n",
       " (u'location', 292),\n",
       " (u'minute', 291),\n",
       " (u'steak', 288),\n",
       " (u'potato', 287),\n",
       " (u'big', 286),\n",
       " (u'pork', 286),\n",
       " (u'soup', 285),\n",
       " (u'enough', 285),\n",
       " (u'tried', 283),\n",
       " (u'see', 282),\n",
       " (u'perfect', 281),\n",
       " (u'beef', 280),\n",
       " (u'bread', 279),\n",
       " (u'spicy', 277),\n",
       " (u'shrimp', 274),\n",
       " (u'breakfast', 271),\n",
       " (u'nothing', 269),\n",
       " (u'green', 268),\n",
       " (u'patio', 261),\n",
       " (u'excellent', 260),\n",
       " (u'5', 260),\n",
       " (u'feel', 259),\n",
       " (u'took', 257),\n",
       " (u'awesome', 255),\n",
       " (u'home', 255),\n",
       " (u'need', 254),\n",
       " (u'quite', 253),\n",
       " (u'long', 249),\n",
       " (u'served', 246),\n",
       " (u'beer', 241),\n",
       " (u'looking', 238),\n",
       " (u'dessert', 236),\n",
       " (u'3', 235),\n",
       " (u'egg', 235),\n",
       " (u'enjoyed', 234),\n",
       " (u'worth', 233),\n",
       " (u'visit', 233),\n",
       " (u'roll', 231),\n",
       " (u'full', 229),\n",
       " (u'fried', 229),\n",
       " (u'inside', 227),\n",
       " (u'look', 227),\n",
       " (u'spot', 225),\n",
       " (u'portion', 223),\n",
       " (u'thought', 222),\n",
       " (u'appetizer', 221),\n",
       " (u'probably', 220),\n",
       " (u'sushi', 220),\n",
       " (u'kind', 219),\n",
       " (u'burrito', 218),\n",
       " (u'huge', 217),\n",
       " (u'half', 217),\n",
       " (u'fish', 217),\n",
       " (u'away', 216),\n",
       " (u'maybe', 216),\n",
       " (u'asked', 213),\n",
       " (u'super', 211),\n",
       " (u'different', 209),\n",
       " (u'large', 208),\n",
       " (u'loved', 207),\n",
       " (u'red', 205),\n",
       " (u'item', 205),\n",
       " (u'dining', 205),\n",
       " (u'work', 204),\n",
       " (u'eating', 204),\n",
       " (u'let', 203),\n",
       " (u'waiter', 203),\n",
       " (u'found', 201),\n",
       " (u'husband', 201),\n",
       " (u'wanted', 201),\n",
       " (u'top', 201),\n",
       " (u'outside', 201)]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#print most common words and identify words to replace\n",
    "words=[item for sublist in review_tokenize for item in sublist]\n",
    "nltk.FreqDist(words).most_common(200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_We used the above cell iteratively to identify new positive and negative words after running the cell below._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#create word lists by hand using word frequencies/intuition\n",
    "positive_vocab = [\n",
    "    u'good', u'nice', u'great', u'awesome', u'outstanding',\n",
    "    u'fantastic', u'terrific', u':)', u':-)', u'like', u'love',\n",
    "    u'cute',u'tasty',u'happy',u'delicious',u'fresh',u'favorite',u'excellent',u'enjoy',u'wonderful']\n",
    "negative_vocab = [\n",
    "    u'bad', u'terrible', u'poor', u'crap', u'useless', u'hate', u':(', u':-(',u'wrong']\n",
    "\n",
    "#Replace positive words and negative words\n",
    "for number, review in enumerate(review_tokenize):\n",
    "    for index, word in enumerate(review):\n",
    "        if word in positive_vocab:\n",
    "            review_tokenize[number][index]='Positive Word'\n",
    "        elif word in negative_vocab:\n",
    "            review_tokenize[number][index]='Negative Word'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#create feature POS list\n",
    "feature_pos=[['JJ','NN'],['JJ','NNS'],['RB','JJ'],['RBR','JJ'],['RBS','JJ'],['JJ','JJ'],['NN','JJ'],['NNS','JJ'],['RB','VB'],\n",
    "            ['RR','VB'],['RBS','VB'],['RB','VBD'],['RR','VBD'],['RBS','VBD'],['RB','VBN'],['RR','VBN'],['RBS','VBN'],\n",
    "            ['RB','VBG'],['RR','VBG'],['RBS','VBG']]\n",
    "\n",
    "#get bigram features for each review\n",
    "bigram_features=[]\n",
    "for review in review_tokenize:\n",
    "    rev_parts=nltk.pos_tag(review)\n",
    "    rev_bigrams=list(nltk.ngrams(rev_parts,2))\n",
    "    features=[]\n",
    "    for item1,item2 in rev_bigrams:\n",
    "        if [item1[1],item2[1]] in feature_pos and item1[0]!='Positive Word' and item1[1]!='Positive Word'and item1[0]!='Negative Word' and item1[1]!='Negative Word':\n",
    "            features.append([item1[0],item2[0]])\n",
    "    bigram_features.append(features)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#calculate total negative and total positive words\n",
    "tot_neg=0\n",
    "tot_pos=0\n",
    "for review in review_tokenize:\n",
    "    for word in review:\n",
    "        if word=='Positive Word':\n",
    "            tot_pos+=1\n",
    "        if word=='Negative Word':\n",
    "            tot_neg+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#get the PMI for each bigram feature for each review\n",
    "all_review_pmi = []\n",
    "\n",
    "#iterate over reviews\n",
    "for num, review in enumerate(bigram_features):\n",
    "    review_pmi = []\n",
    "    #iterate over bigrams in each review\n",
    "    for idx, bigram in enumerate(review):\n",
    "        co_pos=1\n",
    "        co_neg=1\n",
    "        #for each bigram, look for occurence in every review and count if 'positive word' or 'negative word' is nearby\n",
    "        for rev in review_tokenize:\n",
    "            length = len(rev)\n",
    "            rnge = range(length-1)\n",
    "            for index in rnge:\n",
    "                if rev[index] == bigram[0] and rev[index+1] == bigram[1]:\n",
    "                    if (index+6) > rnge:\n",
    "                        upper_limit = rnge\n",
    "                    else:\n",
    "                        upper_limit = index+6\n",
    "                    if (index-5) < 0:\n",
    "                        lower_limit = 0\n",
    "                    else:\n",
    "                        lower_limit = index-5\n",
    "                    if 'Positive Word' in rev[lower_limit:upper_limit]:\n",
    "                        co_pos += 1\n",
    "                    if 'Negative Word' in rev[lower_limit:upper_limit]:\n",
    "                        co_neg += 1\n",
    "        #turn counts into PMI\n",
    "        pmi=math.log(float((co_pos +1 ) * tot_neg)/float((co_neg + 1) * tot_pos))\n",
    "        review_pmi.append(pmi)\n",
    "    all_review_pmi.append(review_pmi)       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "semantic_orientation=[]\n",
    "#get the semantic orientation for each review\n",
    "for review in all_review_pmi:\n",
    "    SO=sum(review)\n",
    "    semantic_orientation.append(SO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.675555555556\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda2\\envs\\hitesh27\\lib\\site-packages\\ipykernel_launcher.py:3: FutureWarning: reshape is deprecated and will raise in a subsequent release. Please use .values.reshape(...) instead\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "C:\\ProgramData\\Anaconda2\\envs\\hitesh27\\lib\\site-packages\\sklearn\\cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "#create training and test sets. Build model. Test.\n",
    "yelp_sample['semantic_orientation']=semantic_orientation\n",
    "X=yelp_sample['semantic_orientation'].reshape(len(yelp_sample['semantic_orientation']), 1)\n",
    "Y=list(yelp_sample['rating_class'])\n",
    "from sklearn.cross_validation import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.3, random_state=1)\n",
    "\n",
    "logistic_model4 = LogisticRegression()\n",
    "logistic_model4.fit(X_train, y_train)\n",
    "\n",
    "predict_logistic4 = logistic_model4.predict(X_test)\n",
    "print metrics.accuracy_score(y_test, predict_logistic4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.39      0.03      0.06       287\n",
      "          1       0.68      0.98      0.80       613\n",
      "\n",
      "avg / total       0.59      0.68      0.57       900\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(metrics.classification_report(y_test, predict_logistic4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_This accuracy is only a small improvement over the baseline (.6733333). There are a couple of refinements that might improve accuracy (e.g., incoporation of unigram features,testing variations of proximity cutoff,etc.), but these results suggest that semantic orientation may not have a clear relationship with whether a review is above 4 or not. The baseline also suggests that review are generally positive, which is reaffirmed by the abundance of positive words relative to negative words._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Task F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What are the top 5 “attributes” of a restaurant that are associated with (i) high and (ii) low ratings? That is, when people rate a restaurant high or low, are they more likely to mention service, ambiance, etc.? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "yelp_sample['review_words'] = review_tokenize\n",
    "\n",
    "mask_high=(yelp_sample['rating_class'] == 1)\n",
    "mask_low=(yelp_sample['rating_class'] == 0)\n",
    "\n",
    "high_reviews=list(yelp_sample['review_words'][mask_high])\n",
    "low_reviews=list(yelp_sample['review_words'][mask_low])         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'wa', 2848),\n",
       " (u'food', 1604),\n",
       " (u'place', 1592),\n",
       " (u'time', 865),\n",
       " (u'service', 675),\n",
       " (u'restaurant', 539),\n",
       " (u'menu', 471),\n",
       " (u'order', 385),\n",
       " (u'thing', 346),\n",
       " (u'chicken', 339),\n",
       " (u'lunch', 337),\n",
       " (u'ha', 326),\n",
       " (u'price', 311),\n",
       " (u'burger', 310),\n",
       " (u'staff', 300),\n",
       " (u'night', 298),\n",
       " (u'meal', 295),\n",
       " (u'sauce', 292),\n",
       " (u'hour', 287),\n",
       " (u'bar', 284)]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "high_words=[]\n",
    "for review in high_reviews:\n",
    "    rev_parts=nltk.pos_tag(review)\n",
    "    for part in rev_parts:\n",
    "        if part[1]=='NN' or part[1]=='NNS':\n",
    "            high_words.append(part[0])\n",
    "most_common_words = nltk.FreqDist(high_words).most_common(20)          \n",
    "nltk.FreqDist(high_words).most_common(20) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'wa', 2179),\n",
       " (u'food', 916),\n",
       " (u'place', 773),\n",
       " (u'time', 462),\n",
       " (u'service', 410),\n",
       " (u'restaurant', 332),\n",
       " (u'order', 302),\n",
       " (u'menu', 223),\n",
       " (u'thing', 217),\n",
       " (u'way', 204),\n",
       " (u'lunch', 191),\n",
       " (u'people', 181),\n",
       " (u'chicken', 178),\n",
       " (u'price', 176),\n",
       " (u'star', 172),\n",
       " (u'meal', 163),\n",
       " (u'nothing', 160),\n",
       " (u'something', 157),\n",
       " (u'sauce', 150),\n",
       " (u'burger', 147)]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "low_words=[]\n",
    "for review in low_reviews:\n",
    "    rev_parts=nltk.pos_tag(review)\n",
    "    for part in rev_parts:\n",
    "        if part[1]=='NN' or part[1]=='NNS':\n",
    "            low_words.append(part[0])\n",
    "nltk.FreqDist(low_words).most_common(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[((u'wa', 2848), (u'wa', 2179)),\n",
       " ((u'food', 1604), (u'food', 916)),\n",
       " ((u'place', 1592), (u'place', 773)),\n",
       " ((u'time', 865), (u'time', 462)),\n",
       " ((u'service', 675), (u'service', 410)),\n",
       " ((u'restaurant', 539), (u'restaurant', 332)),\n",
       " ((u'menu', 471), (u'order', 302)),\n",
       " ((u'order', 385), (u'menu', 223)),\n",
       " ((u'thing', 346), (u'thing', 217)),\n",
       " ((u'chicken', 339), (u'way', 204)),\n",
       " ((u'lunch', 337), (u'lunch', 191)),\n",
       " ((u'ha', 326), (u'people', 181)),\n",
       " ((u'price', 311), (u'chicken', 178)),\n",
       " ((u'burger', 310), (u'price', 176)),\n",
       " ((u'staff', 300), (u'star', 172)),\n",
       " ((u'night', 298), (u'meal', 163)),\n",
       " ((u'meal', 295), (u'nothing', 160)),\n",
       " ((u'sauce', 292), (u'something', 157)),\n",
       " ((u'hour', 287), (u'sauce', 150)),\n",
       " ((u'bar', 284), (u'burger', 147))]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "most_common_words = zip(most_common_words, nltk.FreqDist(low_words).most_common(20))\n",
    "most_common_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_The most common terms in the top 10 are common amongst positive as well as negative reviews - food, place, time, service, menu, order, lunch, chicken/burger/salad/sauce._\n",
    "\n",
    "_The most common terms describe the food. Service and price are mentioned next most often._\n",
    "\n",
    "_Experience and people related to ambience crops up into the top 20 negative reviews only._\n",
    "\n",
    "\n",
    "_So our interpretation is that people are equally likely to write positively or negatively about food, service and price. But they tend to criticize the experience and ambience more often._"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
